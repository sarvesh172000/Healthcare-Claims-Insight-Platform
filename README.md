# ðŸ¥ Healthcare Claims ETL Pipeline

An end-to-end data engineering project to process large volumes of healthcare insurance claims, detect fraud, and optimize cost. Built with a modern stack:

**Apache NiFi â†’ AWS S3 â†’ PySpark â†’ DBT â†’ Snowflake â†’ Airflow â†’ Power BI**

---

## ðŸ“¦ Architecture Overview

```text
[NiFi] â†’ [S3 Raw Layer] â†’ [PySpark ETL] â†’ [S3 Processed Layer] â†’ [DBT] â†’ [Snowflake] â†’ [Power BI Dashboard]
                      â†˜ [Airflow Orchestration & Scheduling] â†™
```

---

## ðŸ› ï¸ Stack

| Component       | Technology     |
|----------------|----------------|
| Ingestion      | Apache NiFi    |
| Raw Storage    | AWS S3         |
| Processing     | PySpark        |
| Modeling       | DBT            |
| Warehouse      | Snowflake      |
| Orchestration  | Apache Airflow |
| Dashboarding   | Power BI       |
| Containerization| Docker & Compose |
| CI/CD          | GitHub Actions |

---

## ðŸ“ Directory Structure

```bash
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ nifi/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ flow/            # Optional flow template or flow.json.gz
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ spark_etl.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ healthcare_project/
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â””â”€â”€ models/
â”‚           â””â”€â”€ patient_summary.sql
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ healthcare_etl_dag.py
â””â”€â”€ .github/workflows/ci.yml
```

---

## ðŸš€ How to Run Locally

### Prerequisites
- Docker & Docker Compose
- AWS credentials for S3 access (via `.env` file)
- Snowflake account credentials

### Steps
```bash
git clone https://github.com/your-repo/healthcare-etl
cd healthcare-etl
docker compose up --build
```

Access:
- **NiFi UI**: http://localhost:8081
- **Airflow UI**: http://localhost:8080

---

## âš™ï¸ Services Details

### ðŸ”„ NiFi
- Ingests CSV files and uploads to `s3a://healthcare-etl-raw/staging/`
- Uses processors: `GetFile`, `PutS3Object`

### ðŸ”¥ PySpark
```python
# spark/spark_etl.py
from pyspark.sql import SparkSession
...
df = spark.read.option("header", True).csv("s3a://healthcare-etl-raw/staging/healthcare_data.csv")
...
df_clean.write.mode("overwrite").parquet("s3a://healthcare-etl-cleaned/processed/")
```

### ðŸ“Š DBT
- Connects to Snowflake
- Models built in `models/patient_summary.sql`

### â± Airflow
```python
# airflow/dags/healthcare_etl_dag.py
run_spark_etl >> run_dbt
```

### ðŸ“‰ Power BI
- Connect to Snowflake with ODBC or native connector
- Example dashboard:
  - Fraudulent Claims Count
  - Avg. Cost per Diagnosis
  - Provider vs Payer Ratios

---

## ðŸ§ª CI/CD

### `.github/workflows/ci.yml`
- Lint `spark_etl.py` with `flake8`
- Build Docker images

---

## ðŸ™Œ Contribution
PRs are welcome! Raise issues if you need help setting up.

---

## ðŸ“„ License
MIT License
